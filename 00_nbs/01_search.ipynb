{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "224b597e-f55b-4fd5-85f0-f3595211de95",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50219bce-9c91-4cf3-8530-d0507933e848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f317ff83-5b2e-451b-a5bb-5268ffe2f75a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev\n",
      "Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dima/anaconda3/envs/f4/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                not been set for this class (UnsupervisedMetrics). The property determines if `update` by\n",
      "                default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                achieved and we recommend setting this to `False`.\n",
      "                We provide an checking function\n",
      "                `from torchmetrics.utilities import check_forward_no_full_state`\n",
      "                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                default for now) or if `full_state_update=False` can be used safely.\n",
      "                \n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dima/FoodSeg103-Benchmark-v1/checkpoints/SETR_Naive_ReLeM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dima/FoodSeg103-Benchmark-v1/mmseg/models/builder.py:59: UserWarning: train_cfg and test_cfg is deprecated, please specify them in model\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load pre-trained weight from imagenet21k\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "from mytools.tools import *\n",
    "from food.paths import *\n",
    "from food.psql import *\n",
    "import requests\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFont, ImageDraw, ImageEnhance\n",
    "from food.paths import *\n",
    "from stego.segment import get_food_segment\n",
    "from segmentor.segment import *\n",
    "\n",
    "from mytools.visual import *\n",
    "from food.depth import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bfc8272-4f6c-4191-9ff3-17d3b35df463",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap,LinearSegmentedColormap\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95a9ad7b-d74f-4e41-86e5-80f7532fa283",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def apply_custom_colormap(image_gray, cmap):\n",
    "\n",
    "    assert image_gray.dtype == np.uint8, 'must be np.uint8 image'\n",
    "\n",
    "    # Initialize the matplotlib color map\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap)\n",
    "\n",
    "    # Obtain linear color range\n",
    "    color_range = sm.to_rgba(np.linspace(0, 1, 256))[:,0:3]    # color range RGBA => RGB\n",
    "    color_range = (color_range*255.0).astype(np.uint8)         # [0,1] => [0,255]\n",
    "    # color_range = np.squeeze(np.dstack([color_range[:,2], color_range[:,1], color_range[:,0]]), 0)  # RGB => BGR\n",
    "\n",
    "    # Apply colormap for each channel individually\n",
    "    channels = [cv2.LUT(image_gray, color_range[:,i]) for i in range(3)]\n",
    "    return np.dstack(channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f607c982-7b2c-44a8-b786-c929f1d19a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_heatmap(arr,\n",
    "                colors = [\"white\",\"lime\",\"green\",\"yellow\",\"orange\", \"red\",\"purple\"],\n",
    "                values = [0,1,50,100,200,300,400]):\n",
    "    \n",
    "    l = list(zip([v/max(values) for v in values],colors))\n",
    "    cmap=LinearSegmentedColormap.from_list('hmap',l)\n",
    "    return apply_custom_colormap((np.array(arr)/max(values)*255).astype(np.uint8),cmap)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3040b433-16c3-4484-aa60-f70cedc16250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend_array2img(img,arr,alphas = [0.5,0.5]):\n",
    "    return cv2.addWeighted(arr, alphas[0], np.array(img).astype(np.uint8), alphas[1], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0d95457-c328-479f-8212-4e4c1dec94e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dima/mytools/mytools/tools.py:27: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  series2tensor = lambda series:torch.tensor([np.array(c) for c in series.values])\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "bad_cats  = ['Vegetables on a sandwich','Candy containing chocolate','Baby juice']\n",
    "bad_descs = ['Banana, fried']\n",
    "\n",
    "\n",
    "foods = pd.read_sql(\"\"\"select f.*\n",
    "                        from food.foods_prompted f\"\"\",engine)\n",
    "\n",
    "# foods = foods.drop(columns = ['clip'])\n",
    "foods = foods.set_index('id')\n",
    "foods = foods.dropna()\n",
    "\n",
    "foods = foods[~foods['category'].isin(bad_cats)]\n",
    "foods = foods[~foods['description'].isin(bad_descs)]\n",
    "\n",
    "food_clips = series2tensor(foods['clip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d782ee3c-958a-45c4-9e90-0242b87941c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calories = energy_mask/100*0.001*20\n",
    "# weight = np.where(energy_mask!=0,1,0)\n",
    "# (weight*0.001*30).sum()\n",
    "# calories.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dd61d70-91fe-4ecf-826a-3e0f79c03a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def search(url):\n",
    "    img = get_image_from_url(url)\n",
    "    img,adj = crop_image_to_square(img,True)\n",
    "    x_adj,y_adj,size = adj\n",
    "    photo_id = url.split('/')[-1]\n",
    "    i = np.asarray(img, dtype=\"uint8\")\n",
    "    i = np.flip(i,2)\n",
    "    segmentor_mask = inference_segmentor(segment_model, i)[0]\n",
    "\n",
    "    classes = np.unique(segmentor_mask)[1:]\n",
    "    classes_ =[]\n",
    "    urls = []\n",
    "    for c in classes:\n",
    "        area = segmentor_mask[segmentor_mask==c].shape[0]\n",
    "        if area> 20*20:\n",
    "            class_mask = np.where(segmentor_mask==c,1,0)\n",
    "            class_mask = expand_boundaries(class_mask,times=5,factor=5)\n",
    "            img_arr = apply_mask(img,class_mask.T).astype(np.uint8)\n",
    "            img_arr = crop_zeros(img_arr)\n",
    "            img_arr[img_arr==[0,0,0]]=255 #replace black with while\n",
    "            fname = f'{photo_id}_{c}.jpg'\n",
    "\n",
    "            Image.fromarray(img_arr).save(fragment_reference_images_path/fname)\n",
    "            urls.append(f'https://dima.grankin.eu/fragment_reference_images/{fname}')\n",
    "            classes_.append(c+1)\n",
    "\n",
    "    classes = classes_\n",
    "    stego_img,stego_mask = get_food_segment(img)\n",
    "    stego_img.save(fragment_reference_images_path/f'{photo_id}_stego.jpg')\n",
    "    urls.append(f'https://dima.grankin.eu/fragment_reference_images/{photo_id}_stego.jpg')\n",
    "\n",
    "    clip_df = pd.DataFrame()\n",
    "    for u in urls:\n",
    "        clip_df = clip_df.append(search_clip(u,foods,food_clips,head = 1)[1])\n",
    "    clip_df=clip_df.reset_index(drop=True)\n",
    "    clip_df['classes'] = classes+[1]\n",
    "    clip_df=clip_df[clip_df['score']>0.22]\n",
    "\n",
    "    mask = stego_mask+segmentor_mask\n",
    "\n",
    "    dicts =[]\n",
    "    masks =[]\n",
    "\n",
    "    attributes = ['energy','protein','carb','fat']\n",
    "    #create masks of attributes\n",
    "    for col in attributes:\n",
    "        dicts.append(clip_df[['classes',col]].set_index(\"classes\")[col].to_dict())\n",
    "        masks.append(torch.clone(mask))\n",
    "\n",
    "    areas = {}\n",
    "    for c in np.unique(mask):\n",
    "        areas[c]= mask[mask==c].shape[0]\n",
    "\n",
    "        #clean values where classes are filtered out\n",
    "        if c not in dicts[0].keys():\n",
    "            for m in masks:\n",
    "                m[m==c]=0\n",
    "\n",
    "    #areas          \n",
    "    clip_df = clip_df.merge(pd.DataFrame(areas,index = ['area']).T,left_on = 'classes',right_index = True)\n",
    "    clip_df['area'] = clip_df['area']/clip_df['area'].sum()\n",
    "    clip_df = clip_df.sort_values('area',ascending = False)\n",
    "\n",
    "    #assign values to the masks\n",
    "    for d,m in zip(dicts,masks):\n",
    "        for k,v in d.items(): m[m == k] = v\n",
    "\n",
    "    stats = pd.DataFrame([float(m[m!=0].mean()) for m in masks]+[masks[0][masks[0]!=0].shape[0]],\n",
    "                     index = attributes+['size'])\n",
    "    blended_img = blend_array2img(img,get_heatmap(masks[0]))\n",
    "    \n",
    "    \n",
    "    img = Image.fromarray(blended_img[-y_adj:size+y_adj,-x_adj:size+x_adj,:])\n",
    "    \n",
    "\n",
    "    return img,clip_df,masks,urls,stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b9c6c1-2ea5-46b2-9a71-2f3cf7f657cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eea0c6af-e497-4cfc-82ad-c079daeae677",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://image.shutterstock.com/z/stock-photo-baked-ribs-with-french-fries-and-cabbage-salad-1020686221.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5b40aec-79a1-4c60-9a8f-88e45144a22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://dima.grankin.eu/reference_images/AgACAgIAAxkBAAIKFWLjz26jI4-aVIsTZmHon_Zlb7rrAAJBxDEbFPIZSw3jSOUNCySqAQADAgADeQADKQQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99a5abe5-12be-4efb-b73f-97e8c2b97450",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://image.shutterstock.com/z/stock-photo-burger-and-fried-potatoes-isolated-on-white-background-top-view-1008179509.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8de5407-7b56-40ad-a4d5-23057801fa4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dima/anaconda3/envs/f4/lib/python3.9/site-packages/torch/nn/functional.py:3509: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home/dima/anaconda3/envs/f4/lib/python3.9/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\n",
      "/home/dima/anaconda3/envs/f4/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/dima/anaconda3/envs/f4/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "img,clip_df,masks,urls,stats = search(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47ddbde-eae3-41b2-bf99-5a3e90052e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc3f69d-67ef-4322-b03a-acf55c907d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.T.astype(int).drop(columns = ['size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7a500a-2783-4b50-b037-512ebb20babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6e9ba6-671f-4526-b94b-2de6073c4a26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db116997-c8e3-4093-916c-d68f4ec577d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4a64b3da-11af-4c16-826b-d824a2fa23a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dishes = pd.DataFrame(dishes,columns = ['amount','energy','protein'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6932619b-64e9-4cf8-b67a-f430b16f4c8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mean_energy  \u001b[38;5;241m=\u001b[39m (\u001b[43mdishes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m*\u001b[39mdishes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menergy\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m/\u001b[39mdishes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamount\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m      2\u001b[0m mean_protein \u001b[38;5;241m=\u001b[39m (dishes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamount\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39mdishes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprotein\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m/\u001b[39mdishes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamount\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "mean_energy  = (dishes['amount']*dishes['energy']).sum()/dishes['amount'].sum()\n",
    "mean_protein = (dishes['amount']*dishes['protein']).sum()/dishes['amount'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c5badd7-143e-4157-a2f6-803debc36915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_paths.ipynb.\n",
      "Converted 00_psql.ipynb.\n",
      "Converted 01_search.ipynb.\n",
      "Converted bot.ipynb.\n",
      "Converted depth.ipynb.\n",
      "Converted inference.ipynb.\n",
      "Converted search_dev.ipynb.\n",
      "Converted search_segmented_stable.ipynb.\n",
      "Converted stats.ipynb.\n",
      "Converted usda.ipynb.\n",
      "Converted usda_parsing.ipynb.\n"
     ]
    }
   ],
   "source": [
    "!conda activate f4; nbdev_build_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24703207-aefd-43eb-bc9b-e29acc624741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f4",
   "language": "python",
   "name": "f4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
