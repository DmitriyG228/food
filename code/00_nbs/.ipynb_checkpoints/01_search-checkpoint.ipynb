{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "224b597e-f55b-4fd5-85f0-f3595211de95",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50219bce-9c91-4cf3-8530-d0507933e848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f317ff83-5b2e-451b-a5bb-5268ffe2f75a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "from mytools.tools import *\n",
    "from food.paths import *\n",
    "from food.psql import *\n",
    "import requests\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFont, ImageDraw, ImageEnhance,ImageOps\n",
    "from food.paths import *\n",
    "# from stego.segment import get_food_segment\n",
    "from segmentor.segment import *\n",
    "\n",
    "from mytools.visual import *\n",
    "from food.depth import *\n",
    "\n",
    "from matplotlib.colors import ListedColormap,LinearSegmentedColormap\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95a9ad7b-d74f-4e41-86e5-80f7532fa283",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def apply_custom_colormap(image_gray, cmap):\n",
    "\n",
    "    assert image_gray.dtype == np.uint8, 'must be np.uint8 image'\n",
    "\n",
    "    # Initialize the matplotlib color map\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap)\n",
    "\n",
    "    # Obtain linear color range\n",
    "    color_range = sm.to_rgba(np.linspace(0, 1, 256))[:,0:3]    # color range RGBA => RGB\n",
    "    color_range = (color_range*255.0).astype(np.uint8)         # [0,1] => [0,255]\n",
    "    # color_range = np.squeeze(np.dstack([color_range[:,2], color_range[:,1], color_range[:,0]]), 0)  # RGB => BGR\n",
    "\n",
    "    # Apply colormap for each channel individually\n",
    "    channels = [cv2.LUT(image_gray, color_range[:,i]) for i in range(3)]\n",
    "    return np.dstack(channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f607c982-7b2c-44a8-b786-c929f1d19a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_heatmap(arr,\n",
    "                colors = [\"white\",\"lime\",\"green\",\"yellow\",\"orange\", \"red\",\"purple\",\"purple\",\"purple\",\"purple\",\"purple\",\"purple\"],\n",
    "                values = [0,           1,     50,     100,     200,   300,     400,     500,     600,     700,     800,     900]):\n",
    "    \n",
    "    l = list(zip([v/max(values) for v in values],colors))\n",
    "    cmap=LinearSegmentedColormap.from_list('hmap',l)\n",
    "    return apply_custom_colormap((np.array(arr)/max(values)*255).astype(np.uint8),cmap)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3040b433-16c3-4484-aa60-f70cedc16250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def blend_array2img(img,arr,alphas = [0.5,0.5]):\n",
    "    return cv2.addWeighted(arr, alphas[0], np.array(img).astype(np.uint8), alphas[1], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0d95457-c328-479f-8212-4e4c1dec94e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dima/mytools/mytools/visual.py:26: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  series2tensor = lambda series:torch.tensor([np.array(c) for c in series.values])\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "bad_cats  = ['Vegetables on a sandwich','Candy containing chocolate','Baby juice']\n",
    "bad_descs = ['Banana, fried']\n",
    "bad_keys = ['baby food','frozen','juice','drink']\n",
    "bad_keys_cat = ['formula']\n",
    "\n",
    "q = \"\"\"select p.clip, p.text, p.version,f.description,f.category, f.id,f.energy,f.protein,f.carb,f.fat\n",
    "       from food.foods_prompted as p\n",
    "       join food.foods as f on (p.food_id = f.id)\n",
    "       where p.clip is not null\n",
    "       \"\"\"\n",
    "\n",
    "\n",
    "foods = pd.read_sql(q,engine)\n",
    "\n",
    "# foods = foods.drop(columns = ['clip'])\n",
    "foods = foods.set_index('id')\n",
    "foods = foods.dropna()\n",
    "\n",
    "foods = foods[~foods['category'].isin(bad_cats)]\n",
    "foods = foods[~foods['description'].isin(bad_descs)]\n",
    "foods = foods[~foods['description'].str.contains('|'.join(bad_keys))]\n",
    "foods = foods[~foods['category']   .str.lower().str.contains('|'.join(bad_keys_cat))]\n",
    "\n",
    "food_clips = series2tensor(foods['clip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "779dbb6c-1625-43dd-ae12-a1b4bfa42a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'https://dima.skynet.center'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f24b77bd-ce20-4d34-bc82-9e15940ab0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "### depr\n",
    "def search(segment_model,url=None,path=None,stego = False, prompt_factor=0.5,min_score=0.22,exand_times =2):\n",
    "    img = get_image(url=url,path=path)\n",
    "    img,adj = crop_image_to_square(img,True)\n",
    "    x_adj,y_adj,size = adj\n",
    "    \n",
    "    photo_id = url if url else path.name\n",
    "    photo_id = photo_id.split('/')[-1]\n",
    "    \n",
    "    i = np.asarray(img, dtype=\"uint8\")\n",
    "    i = np.flip(i,2)\n",
    "    segmentor_mask = inference_segmentor(segment_model, i)[0]\n",
    "    segmentor_mask[segmentor_mask!=0]=segmentor_mask[segmentor_mask!=0]+1 \n",
    "\n",
    "\n",
    "\n",
    "    classes = np.unique(segmentor_mask)[1:]\n",
    "    classes_ =[]\n",
    "    urls = []\n",
    "    for c in classes:\n",
    "        area = segmentor_mask[segmentor_mask==c].shape[0]\n",
    "        if area> 20*20:\n",
    "            class_mask = np.where(segmentor_mask==c,1,0)\n",
    "            class_mask = expand_boundaries(class_mask,times=exand_times,factor=10)\n",
    "            img_arr = apply_mask(img,class_mask.T).astype(np.uint8)\n",
    "            img_arr = crop_zeros(img_arr)\n",
    "            img_arr[img_arr==[0,0,0]]=255 #replace black with while\n",
    "            fname = f'{photo_id}_{c}.jpg'\n",
    "            Image.fromarray(img_arr).save(fragment_reference_images_path/fname)\n",
    "            urls.append(f'{domain}/fragment_reference_images/{fname}')\n",
    "            classes_.append(c)\n",
    "    classes = classes_\n",
    "\n",
    "\n",
    "    if stego:\n",
    "\n",
    "        stego_img,stego_mask = get_food_segment(img)\n",
    "\n",
    "        s = np.copy(segmentor_mask)\n",
    "        s[s!=0] = 1\n",
    "        inverse_stego_mask = stego_mask - s\n",
    "        inverse_stego_mask[inverse_stego_mask==-1]=0\n",
    "        inverse_stego_img = Image.fromarray(apply_mask(img,inverse_stego_mask).astype(np.uint8))\n",
    "        ##new\n",
    "        stego_img        .save(fragment_reference_images_path/f'{photo_id}_stego.jpg')\n",
    "        inverse_stego_img.save(fragment_reference_images_path/f'{photo_id}_inverse_stego.jpg')\n",
    "        urls.append(f'{domain}/fragment_reference_images/{photo_id}_inverse_stego.jpg')\n",
    "\n",
    "    #to push segmented clips towards the whole dish clip\n",
    "\n",
    "    main_image_url = f'{domain}/fragment_reference_images/{photo_id}_stego.jpg' if stego else url\n",
    "    main_image_clip = get_image_clip(main_image_url)\n",
    "\n",
    "    clip_df = pd.DataFrame()\n",
    "    for u in urls:\n",
    "        df = search_clip(u,foods,food_clips,prompt_clip=main_image_clip,head = 1,prompt_factor=prompt_factor)[1]\n",
    "        # df['url'] = u\n",
    "        clip_df = clip_df.append(df)\n",
    "    clip_df=clip_df.reset_index(drop=True)\n",
    "    clip_df['classes'] = classes+[1] if stego else classes\n",
    "    #new\n",
    "\n",
    "    clip_df=clip_df[clip_df['score']>min_score]\n",
    "\n",
    "    mask = torch.Tensor(segmentor_mask)+inverse_stego_mask if stego else torch.Tensor(segmentor_mask)\n",
    "\n",
    "    dicts =[]\n",
    "    masks =[]\n",
    "\n",
    "    attributes = ['energy','protein','carb','fat']\n",
    "    #create masks of attributes\n",
    "    for col in attributes:\n",
    "        dicts.append(clip_df[['classes',col]].set_index(\"classes\")[col].to_dict())\n",
    "        masks.append(torch.clone(mask))\n",
    "\n",
    "    areas = {}\n",
    "    for c in np.unique(mask):\n",
    "        areas[c]= mask[mask==c].shape[0]\n",
    "\n",
    "        #clean values where classes are filtered out\n",
    "        if c not in dicts[0].keys():\n",
    "            for m in masks:\n",
    "                m[m==c]=0\n",
    "\n",
    "    #areas          \n",
    "    clip_df = clip_df.merge(pd.DataFrame(areas,index = ['area']).T,left_on = 'classes',right_index = True)\n",
    "    clip_df = clip_df.sort_values('area',ascending = False)\n",
    "\n",
    "    #assign values to the masks\n",
    "    for d,m in zip(dicts,masks):\n",
    "        for k,v in d.items(): m[m == k] = v\n",
    "\n",
    "    stats = pd.DataFrame([float(m[m!=0].mean()) for m in masks]+[masks[0][masks[0]!=0].shape[0]],\n",
    "                     index = attributes+['size'])\n",
    "\n",
    "\n",
    "    img = ImageOps.grayscale(img).convert('RGB')\n",
    "    blended_img = blend_array2img(img,get_heatmap(masks[0]),alphas=[0.5, 0.9])\n",
    "    img = Image.fromarray(blended_img[-y_adj:size+y_adj,-x_adj:size+x_adj,:])\n",
    "    return img,clip_df,masks,urls,stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b715d7f-eed5-4d0d-b858-ae7f3855aa28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dima/segmentator/mmseg/models/builder.py:59: UserWarning: train_cfg and test_cfg is deprecated, please specify them in model\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load pre-trained weight from imagenet21k\n"
     ]
    }
   ],
   "source": [
    "model_path = checkpoints_path.ls()[0]\n",
    "segment_model = get_segment_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e068b64c-ad79-4e0d-864d-c1b4c45a89c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = reference_images_path.ls()\n",
    "path= paths[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c16f840-7d49-4171-b197-c6fb34d9df68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def search(segment_model,path=None, prompt_factor=0.5,min_score=0.22,exand_times =2):\n",
    "    img = get_image(path=path)\n",
    "    img,adj = crop_image_to_square(img,True)\n",
    "    x_adj,y_adj,size = adj\n",
    "\n",
    "    photo_id = path.name.split('/')[-1]\n",
    "\n",
    "    i = np.asarray(img, dtype=\"uint8\")\n",
    "    i = np.flip(i,2)\n",
    "    segmentor_mask = inference_segmentor(segment_model, i)[0]\n",
    "    segmentor_mask[segmentor_mask!=0]=segmentor_mask[segmentor_mask!=0]+1 \n",
    "    classes = np.unique(segmentor_mask)[1:]\n",
    "    classes_ =[]\n",
    "\n",
    "    paths = []\n",
    "    for c in classes:\n",
    "        area = segmentor_mask[segmentor_mask==c].shape[0]\n",
    "        if area> 20*20:\n",
    "            class_mask = np.where(segmentor_mask==c,1,0)\n",
    "            class_mask = expand_boundaries(class_mask,times=exand_times,factor=10)\n",
    "            img_arr = apply_mask(img,class_mask.T).astype(np.uint8)\n",
    "            img_arr = crop_zeros(img_arr)\n",
    "            img_arr[img_arr==[0,0,0]]=255 #replace black with while\n",
    "            fname = f'{photo_id}_{c}.jpg'\n",
    "            p = fragment_reference_images_path/fname\n",
    "            paths.append(p)\n",
    "            Image.fromarray(img_arr).save(p)\n",
    "            classes_.append(c)\n",
    "    classes = classes_\n",
    "    \n",
    "    if prompt_factor >0: paths = [path]+paths\n",
    "\n",
    "    clips =torch.Tensor(get_image_clip_from_paths(paths))\n",
    "\n",
    "    if prompt_factor>0:\n",
    "        prompt_clip = clips[0]\n",
    "        clips = clips[1:]\n",
    "        diff = prompt_clip - clips\n",
    "        clips = clips + diff*prompt_factor\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    for clip in clips:\n",
    "        df = foods.copy()\n",
    "        df['score'] = cos(food_clips,clip)\n",
    "        dfs.append(df.sort_values('score',ascending=False)[:1])\n",
    "\n",
    "    clip_df = pd.concat(dfs)\n",
    "    clip_df['classes'] = classes\n",
    "\n",
    "    clip_df=clip_df[clip_df['score']>min_score]\n",
    "\n",
    "    mask = torch.Tensor(segmentor_mask)\n",
    "\n",
    "    dicts =[]\n",
    "    masks =[]\n",
    "\n",
    "    attributes = ['energy','protein','carb','fat']\n",
    "    #create masks of attributes\n",
    "    for col in attributes:\n",
    "        dicts.append(clip_df[['classes',col]].set_index(\"classes\")[col].to_dict())\n",
    "        masks.append(torch.clone(mask))\n",
    "\n",
    "    areas = {}\n",
    "    for c in np.unique(mask):\n",
    "        areas[c]= mask[mask==c].shape[0]\n",
    "\n",
    "        #clean values where classes are filtered out\n",
    "        if c not in dicts[0].keys():\n",
    "            for m in masks:\n",
    "                m[m==c]=0\n",
    "\n",
    "    #areas          \n",
    "    clip_df = clip_df.merge(pd.DataFrame(areas,index = ['area']).T,left_on = 'classes',right_index = True)\n",
    "    clip_df = clip_df.sort_values('area',ascending = False)\n",
    "\n",
    "    #assign values to the masks\n",
    "    for d,m in zip(dicts,masks):\n",
    "        for k,v in d.items(): m[m == k] = v\n",
    "\n",
    "    stats = pd.DataFrame([float(m[m!=0].mean()) for m in masks]+[masks[0][masks[0]!=0].shape[0]],\n",
    "                     index = attributes+['size'])\n",
    "\n",
    "\n",
    "    img = ImageOps.grayscale(img).convert('RGB')\n",
    "    blended_img = blend_array2img(img,get_heatmap(masks[0]),alphas=[0.5, 0.9])\n",
    "    img = Image.fromarray(blended_img[-y_adj:size+y_adj,-x_adj:size+x_adj,:])\n",
    "    return img,clip_df,masks,stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a147aee5-76b3-4c59-914a-07dce1a73ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path= reference_images_path.ls()[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d0188de-f70a-4de2-9cd6-721dfd778150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dima/anaconda3/envs/f1/lib/python3.9/site-packages/torch/nn/functional.py:3509: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home/dima/anaconda3/envs/f1/lib/python3.9/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "img,clip_df,masks,stats = search(segment_model,path, prompt_factor=0.1,min_score=0.22,exand_times =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdd1d919-0955-40ec-bc1a-03acda0f4a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0eaee04b-e4e1-464d-a2e1-3bddc286463d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dima/anaconda3/envs/f1/lib/python3.9/site-packages/torch/nn/functional.py:3509: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home/dima/anaconda3/envs/f1/lib/python3.9/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 1.37691 s\n",
       "File: /tmp/ipykernel_396248/3771660930.py\n",
       "Function: search at line 1\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     1                                           def search(segment_model,path=None, prompt_factor=0.5,min_score=0.22,exand_times =2):\n",
       "     2         1        587.0    587.0      0.0      img = get_image(path=path)\n",
       "     3         1      12540.0  12540.0      0.9      img,adj = crop_image_to_square(img,True)\n",
       "     4         1          2.0      2.0      0.0      x_adj,y_adj,size = adj\n",
       "     5                                           \n",
       "     6         1          8.0      8.0      0.0      photo_id = url if url else path.name\n",
       "     7         1          2.0      2.0      0.0      photo_id = photo_id.split('/')[-1]\n",
       "     8                                           \n",
       "     9         1       1447.0   1447.0      0.1      i = np.asarray(img, dtype=\"uint8\")\n",
       "    10         1         52.0     52.0      0.0      i = np.flip(i,2)\n",
       "    11         1     259924.0 259924.0     18.9      segmentor_mask = inference_segmentor(segment_model, i)[0]\n",
       "    12         1       2199.0   2199.0      0.2      segmentor_mask[segmentor_mask!=0]=segmentor_mask[segmentor_mask!=0]+1 \n",
       "    13         1      16631.0  16631.0      1.2      classes = np.unique(segmentor_mask)[1:]\n",
       "    14         1          3.0      3.0      0.0      classes_ =[]\n",
       "    15                                           \n",
       "    16         1          1.0      1.0      0.0      paths = []\n",
       "    17         9         50.0      5.6      0.0      for c in classes:\n",
       "    18         8       8663.0   1082.9      0.6          area = segmentor_mask[segmentor_mask==c].shape[0]\n",
       "    19         8         18.0      2.2      0.0          if area> 20*20:\n",
       "    20         7      13247.0   1892.4      1.0              class_mask = np.where(segmentor_mask==c,1,0)\n",
       "    21         7      97476.0  13925.1      7.1              class_mask = expand_boundaries(class_mask,times=exand_times,factor=10)\n",
       "    22         7     310975.0  44425.0     22.6              img_arr = apply_mask(img,class_mask.T).astype(np.uint8)\n",
       "    23         7     127004.0  18143.4      9.2              img_arr = crop_zeros(img_arr)\n",
       "    24         7       9423.0   1346.1      0.7              img_arr[img_arr==[0,0,0]]=255 #replace black with while\n",
       "    25         7         62.0      8.9      0.0              fname = f'{photo_id}_{c}.jpg'\n",
       "    26         7        377.0     53.9      0.0              p = fragment_reference_images_path/fname\n",
       "    27         7         13.0      1.9      0.0              paths.append(p)\n",
       "    28         7      20655.0   2950.7      1.5              Image.fromarray(img_arr).save(p)\n",
       "    29         7         34.0      4.9      0.0              classes_.append(c)\n",
       "    30         1          2.0      2.0      0.0      classes = classes_\n",
       "    31                                               \n",
       "    32         1          4.0      4.0      0.0      if prompt_factor >0: paths = [path]+paths\n",
       "    33                                           \n",
       "    34         1     188904.0 188904.0     13.7      clips =torch.Tensor(get_image_clip_from_paths(paths))\n",
       "    35                                           \n",
       "    36         1          2.0      2.0      0.0      if prompt_factor>0:\n",
       "    37         1         27.0     27.0      0.0          prompt_clip = clips[0]\n",
       "    38         1         10.0     10.0      0.0          clips = clips[1:]\n",
       "    39         1         33.0     33.0      0.0          diff = prompt_clip - clips\n",
       "    40         1         51.0     51.0      0.0          clips = clips + diff*prompt_factor\n",
       "    41                                           \n",
       "    42         1          2.0      2.0      0.0      dfs = []\n",
       "    43                                           \n",
       "    44         8         57.0      7.1      0.0      for clip in clips:\n",
       "    45         7       4047.0    578.1      0.3          df = foods.copy()\n",
       "    46         7     178941.0  25563.0     13.0          df['score'] = cos(food_clips,clip)\n",
       "    47         7      24825.0   3546.4      1.8          dfs.append(df.sort_values('score',ascending=False)[:1])\n",
       "    48                                           \n",
       "    49         1       2722.0   2722.0      0.2      clip_df = pd.concat(dfs)\n",
       "    50         1        329.0    329.0      0.0      clip_df['classes'] = classes\n",
       "    51                                           \n",
       "    52         1        914.0    914.0      0.1      clip_df=clip_df[clip_df['score']>min_score]\n",
       "    53                                           \n",
       "    54         1        593.0    593.0      0.0      mask = torch.Tensor(segmentor_mask)+inverse_stego_mask if stego else torch.Tensor(segmentor_mask)\n",
       "    55                                           \n",
       "    56         1          1.0      1.0      0.0      dicts =[]\n",
       "    57         1          2.0      2.0      0.0      masks =[]\n",
       "    58                                           \n",
       "    59         1          1.0      1.0      0.0      attributes = ['energy','protein','carb','fat']\n",
       "    60                                               #create masks of attributes\n",
       "    61         5          7.0      1.4      0.0      for col in attributes:\n",
       "    62         4       4267.0   1066.8      0.3          dicts.append(clip_df[['classes',col]].set_index(\"classes\")[col].to_dict())\n",
       "    63         4        673.0    168.2      0.0          masks.append(torch.clone(mask))\n",
       "    64                                           \n",
       "    65         1          1.0      1.0      0.0      areas = {}\n",
       "    66        10      27643.0   2764.3      2.0      for c in np.unique(mask):\n",
       "    67         9       4381.0    486.8      0.3          areas[c]= mask[mask==c].shape[0]\n",
       "    68                                           \n",
       "    69                                                   #clean values where classes are filtered out\n",
       "    70         9         55.0      6.1      0.0          if c not in dicts[0].keys():\n",
       "    71        10         18.0      1.8      0.0              for m in masks:\n",
       "    72         8       1514.0    189.2      0.1                  m[m==c]=0\n",
       "    73                                           \n",
       "    74                                               #areas          \n",
       "    75         1       3176.0   3176.0      0.2      clip_df = clip_df.merge(pd.DataFrame(areas,index = ['area']).T,left_on = 'classes',right_index = True)\n",
       "    76         1        454.0    454.0      0.0      clip_df = clip_df.sort_values('area',ascending = False)\n",
       "    77                                           \n",
       "    78                                               #assign values to the masks\n",
       "    79         5          9.0      1.8      0.0      for d,m in zip(dicts,masks):\n",
       "    80        32       3585.0    112.0      0.3          for k,v in d.items(): m[m == k] = v\n",
       "    81                                           \n",
       "    82         2       3034.0   1517.0      0.2      stats = pd.DataFrame([float(m[m!=0].mean()) for m in masks]+[masks[0][masks[0]!=0].shape[0]],\n",
       "    83         1          2.0      2.0      0.0                       index = attributes+['size'])\n",
       "    84                                           \n",
       "    85                                           \n",
       "    86         1       3042.0   3042.0      0.2      img = ImageOps.grayscale(img).convert('RGB')\n",
       "    87         1      40760.0  40760.0      3.0      blended_img = blend_array2img(img,get_heatmap(masks[0]),alphas=[0.5, 0.9])\n",
       "    88         1       1422.0   1422.0      0.1      img = Image.fromarray(blended_img[-y_adj:size+y_adj,-x_adj:size+x_adj,:])\n",
       "    89         1          3.0      3.0      0.0      return img,clip_df,masks,stats"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f search search(segment_model,path, prompt_factor=0.1,min_score=0.22,exand_times =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82bf41ce-1235-4927-8ac2-f2d86c5d65de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev\n"
     ]
    }
   ],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c5badd7-143e-4157-a2f6-803debc36915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dima/anaconda3/bin/pip\n"
     ]
    }
   ],
   "source": [
    "!which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24703207-aefd-43eb-bc9b-e29acc624741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f1",
   "language": "python",
   "name": "f1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
